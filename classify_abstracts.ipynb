{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35938"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction import text\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import metrics\n",
    "from string import punctuation\n",
    "import numpy as np\n",
    "import random\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "DATASET = \"lpi\"\n",
    "\n",
    "abstractFile = \"abstracts_data/pubmed_ecology_abstracts.txt\"\n",
    "with open(abstractFile) as f:\n",
    "    negatives = ' '.join([line.strip() for line in f])\n",
    "negatives = [paragraph for paragraph in negatives.split('  ') if len(paragraph) > 300]\n",
    "random.shuffle(negatives)\n",
    "len(negatives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "691"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if DATASET.lower() == \"predicts\":\n",
    "    df = pd.read_csv('abstracts_data/unique_id_fields.csv')\n",
    "    positives = list(df['Abstract'].dropna())\n",
    "    positives = [item.lower().replace('abstract', '').replace('unavailable','').strip() for item in positives]\n",
    "elif DATASET.lower() == \"lpi\":\n",
    "    with open(\"abstracts_data/lpi_abstracts.txt\") as f:\n",
    "        positives = []\n",
    "        for line in f:\n",
    "            if line.strip():\n",
    "                positives.append(line.strip().lower().replace(\"abstract\",\"\"))\n",
    "len(positives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def split_data(ids, docs, successes, perc_train, shuffle_data):\n",
    "    \"\"\"Split the document and classification data into training and testing sets.\"\"\"\n",
    "\n",
    "    ndx_shuffle = list(range(len(docs)))\n",
    "    # Shuffle lists\n",
    "    if shuffle_data:\n",
    "        random.shuffle(ndx_shuffle)\n",
    "\n",
    "    num_train = int(len(docs) * perc_train)\n",
    "    docs_train = [docs[i].lower() for i in ndx_shuffle[:num_train]]\n",
    "    docs_test = [docs[i].lower() for i in ndx_shuffle[num_train:]]\n",
    "    successes_train = [successes[i] for i in ndx_shuffle[:num_train]]\n",
    "    successes_test = [successes[i] for i in ndx_shuffle[num_train:]]\n",
    "    ids_train = [ids[i] for i in ndx_shuffle[:num_train]]\n",
    "    ids_test = [ids[i] for i in ndx_shuffle[num_train:]]\n",
    "\n",
    "    return ids_train, ids_test, docs_train, docs_test, successes_train, successes_test\n",
    "\n",
    "\n",
    "def rm_punctuation(string, replacement='', exclude=\"'-'\"):\n",
    "    \"\"\"Remove punctuation from an input string \"\"\"\n",
    "    string = string.replace('-', ' ')  # Always replace hyphen with space\n",
    "    for p in set(list(punctuation)) - set(list(exclude)):\n",
    "        string = string.replace(p, replacement)\n",
    "\n",
    "    string = ' '.join(string.split())  # Remove excess whitespace\n",
    "    return string\n",
    "\n",
    "def train_classifier(\n",
    "        docs_success,\n",
    "        docs_background,\n",
    "        perc_train=1.0,\n",
    "        shuffle_data=True,\n",
    "        ngram_range=(1, 1),\n",
    "        filter_params={},\n",
    "        limit_data=None,\n",
    "        cross_val_folds=10\n",
    "):\n",
    "    \"\"\"\n",
    "    Train a classifier given the classification data in input_file.\n",
    "\n",
    "    @param perc_train: percentage of data points to train from input file (between 0 and 1). 0.4 by default.\n",
    "    @param shuffle_data: whether to shuffle the input data before training. True by default.\n",
    "    @return: the keywords and their corresponding coefficients as ndarrays, sorted by coefficient value.\n",
    "    \"\"\"\n",
    "\n",
    "    docs = docs_success + docs_background\n",
    "    successes = [1 for i in range(len(docs_success))] + [0 for j in range(len(docs_background))]\n",
    "    ids = range(len(docs))\n",
    "\n",
    "    # Remove punctuation from docs\n",
    "    docs = [rm_punctuation(str(_)) for _ in docs]\n",
    "\n",
    "    # Split data into training and test sets\n",
    "    ids_train, ids_test, docs_train, docs_test, y_train, y_test = \\\n",
    "        split_data(ids, docs, successes, perc_train, shuffle_data)\n",
    "\n",
    "    # Initialise vectorizer to convert text documents into matrix of token counts\n",
    "    vect = CountVectorizer(min_df=2, ngram_range=ngram_range, stop_words='english')\n",
    "    # Extract features from training dataset using sparse vectorizer\n",
    "    X_train = vect.fit_transform(docs_train)\n",
    "    print(\"LENGTH OF TEST DOCS\",len(docs_test))\n",
    "\n",
    "    # Logistic regression classifier\n",
    "    lr_classifier = LogisticRegression(penalty='l2')\n",
    "    if cross_val_folds > 0:\n",
    "        cv_precision = cross_val_score(lr_classifier, X_train, y_train, cv=cross_val_folds, scoring='precision')\n",
    "        cv_recall = cross_val_score(lr_classifier, X_train, y_train, cv=cross_val_folds, scoring='recall')\n",
    "        print(\"%d-fold cross validation scores:\\nPrecision:%.4f\\nRecall:%.4f\" %\n",
    "              (cross_val_folds, cv_precision.mean(), cv_recall.mean()))\n",
    "\n",
    "    lr_classifier = LogisticRegression(penalty='l2').fit(X_train, y_train)\n",
    "    \n",
    "    def get_top_feats(feature_names, classifier, plot=True, N=10, bar_height=0.5):\n",
    "        \"\"\"Sort keywords by their coefficients\"\"\"\n",
    "        sorted_feats = np.argsort(classifier.coef_[0])  # Sorted by coefficients (descending)\n",
    "        sorted_coeffs = classifier.coef_[0][sorted_feats]\n",
    "\n",
    "        return sorted_feats, sorted_coeffs\n",
    "\n",
    "    features = np.array(vect.get_feature_names())\n",
    "    feat_ids, coeffs = get_top_feats(features, lr_classifier, plot=False, N=20)\n",
    "\n",
    "    if len(docs_test) > 0:\n",
    "        X_test = vect.transform(docs_test)\n",
    "        # Predict test data\n",
    "        y_test_predicted = lr_classifier.predict(X_test)\n",
    "\n",
    "        print('Classifier has precision %.3f and recall %.3f' % \\\n",
    "            (metrics.precision_score(y_test, y_test_predicted),\n",
    "            metrics.recall_score(y_test, y_test_predicted)))\n",
    "\n",
    "        # Examples of misclassified positives/negatives\n",
    "        positive_misses = [docs_test[i] for i in range(len(docs_test)) if (y_test[i] and not y_test_predicted[i])]\n",
    "        negative_misses = [docs_test[i] for i in range(len(docs_test)) if (not y_test[i] and y_test_predicted[i])]\n",
    "\n",
    "        print('\\nSome positive misses:')\n",
    "\n",
    "        for i in range(5):\n",
    "            try:\n",
    "                print(str(i+1) + ') ' + positive_misses[i] + \"\\n\")\n",
    "            except IndexError:\n",
    "                break\n",
    "\n",
    "        print('\\nSome negative misses:')\n",
    "        for i in range(5):\n",
    "            try:\n",
    "                print(str(i+1) + ') ' + negative_misses[i] + \"\\n\")\n",
    "            except IndexError:\n",
    "                break\n",
    "        print('')\n",
    "\n",
    "    return features[feat_ids], coeffs, lr_classifier, vect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LENGTH OF TEST DOCS 0\n",
      "10-fold cross validation scores:\n",
      "Precision:0.9410\n",
      "Recall:0.9306\n"
     ]
    }
   ],
   "source": [
    "features, coefficients, model, vect = train_classifier(\n",
    "    positives,\n",
    "    negatives[:2000],\n",
    "    perc_train=1.0,\n",
    "    ngram_range=(1, 3),\n",
    "    cross_val_folds=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Bottom features\n",
      "water -1.11698966594\n",
      "health -0.632240832625\n",
      "paper -0.61989555873\n",
      "analysis -0.614726106445\n",
      "control -0.603707585186\n",
      "\n",
      "Top features\n",
      "2004 0.419978915629\n",
      "year 0.467869514905\n",
      "conservation 0.478065956089\n",
      "population 0.594435780407\n",
      "abundance 0.947052859406\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nBottom features\")\n",
    "for f,c in list(zip(features,coefficients))[:5]:\n",
    "    print(f,c)\n",
    "    \n",
    "print(\"\\nTop features\")\n",
    "for f,c in list(zip(features,coefficients))[-5:]:\n",
    "    print(f,c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'20174917-1249'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datetime\n",
    "datetime.datetime.now().strftime(\"%Y%M%d-%H%M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['models/lpi_LR_model.pkl']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save model\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "joblib.dump(vect, \"models/%s_LR_vectorizer.pkl\" % (DATASET.lower()), compress=1)\n",
    "joblib.dump(model, \"models/%s_LR_model.pkl\" % (DATASET.lower()), compress=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.load(\"models/%s_LR_model.pkl\" % (DATASET.lower()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.70930308])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict_proba(vect.transform([\"hello world\"]))[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "IOError",
     "evalue": "[Errno 2] No such file or directory: 'PLoS_One/all_abstracts.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIOError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-229a760a10a1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m                         \u001b[0msuggestions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfileName\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mabstract\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcandidate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0msuggestions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0msuggestions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuggest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvect\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msuggestions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"hello\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-31-229a760a10a1>\u001b[0m in \u001b[0;36msuggest\u001b[0;34m(model, vect)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0msuggestions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mfileName\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfileList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msourceDir\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfileName\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m'introduction'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m                 \u001b[0mcandidate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIOError\u001b[0m: [Errno 2] No such file or directory: 'PLoS_One/all_abstracts.csv'"
     ]
    }
   ],
   "source": [
    "def suggest(model,vect):\n",
    "    suggestions = []\n",
    "    for fileName in fileList[:1000]:\n",
    "        with open(os.path.join(sourceDir,fileName)) as f:\n",
    "            if f.readline().strip().lower()=='introduction':\n",
    "                candidate = f.readline()\n",
    "                if len(candidate)>50:\n",
    "                    prediction = model.predict(vect.transform([rm_punctuation(str(candidate))]))\n",
    "                    if prediction[0]:\n",
    "                        suggestions.append(dict(file=fileName, abstract=candidate))\n",
    "    return suggestions\n",
    "suggestions = suggest(model,vect)\n",
    "pd.DataFrame(suggestions).to_csv('suggested_{0}.csv'.format(DATASET.lower()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plos_abstracts = pd.read_csv(\"PLoS_One/all_abstracts.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>abstract</th>\n",
       "      <th>file</th>\n",
       "      <th>lpi_probability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Invasive candidiases are life threatening oppo...</td>\n",
       "      <td>PLoS_One_2013_Jul_23_8(7)_e69664.txt</td>\n",
       "      <td>0.268799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Multiple sclerosis (MS) is a chronic inflammat...</td>\n",
       "      <td>PLoS_One_2011_Oct_20_6(10)_e26262.txt</td>\n",
       "      <td>0.000257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>There is a strong ongoing debate on the catego...</td>\n",
       "      <td>PLoS_One_2012_Feb_17_7(2)_e30727.txt</td>\n",
       "      <td>0.002204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Assay of the T cell response to antigens of My...</td>\n",
       "      <td>PLoS_One_2013_Aug_8_8(8)_e71351.txt</td>\n",
       "      <td>0.006375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Motor disability is one of the most common def...</td>\n",
       "      <td>PLoS_One_2014_Jan_8_9(1)_e84729.txt</td>\n",
       "      <td>0.010826</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                           abstract  \\\n",
       "0           0  Invasive candidiases are life threatening oppo...   \n",
       "1           1  Multiple sclerosis (MS) is a chronic inflammat...   \n",
       "2           2  There is a strong ongoing debate on the catego...   \n",
       "3           3  Assay of the T cell response to antigens of My...   \n",
       "4           4  Motor disability is one of the most common def...   \n",
       "\n",
       "                                    file  lpi_probability  \n",
       "0   PLoS_One_2013_Jul_23_8(7)_e69664.txt         0.268799  \n",
       "1  PLoS_One_2011_Oct_20_6(10)_e26262.txt         0.000257  \n",
       "2   PLoS_One_2012_Feb_17_7(2)_e30727.txt         0.002204  \n",
       "3    PLoS_One_2013_Aug_8_8(8)_e71351.txt         0.006375  \n",
       "4    PLoS_One_2014_Jan_8_9(1)_e84729.txt         0.010826  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plos_abstracts[\"lpi_probability\"] = model.predict_proba(vect.transform(\n",
    "        [rm_punctuation(str(a)) for a in plos_abstracts.abstract.values]))[:,1]\n",
    "plos_abstracts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>abstract</th>\n",
       "      <th>file</th>\n",
       "      <th>lpi_probability</th>\n",
       "      <th>doi</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Invasive candidiases are life threatening oppo...</td>\n",
       "      <td>PLoS_One_2013_Jul_23_8(7)_e69664.txt</td>\n",
       "      <td>0.268799</td>\n",
       "      <td>http://dx.doi.org/10.1371/journal.pone.69664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Multiple sclerosis (MS) is a chronic inflammat...</td>\n",
       "      <td>PLoS_One_2011_Oct_20_6(10)_e26262.txt</td>\n",
       "      <td>0.000257</td>\n",
       "      <td>http://dx.doi.org/10.1371/journal.pone.26262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>There is a strong ongoing debate on the catego...</td>\n",
       "      <td>PLoS_One_2012_Feb_17_7(2)_e30727.txt</td>\n",
       "      <td>0.002204</td>\n",
       "      <td>http://dx.doi.org/10.1371/journal.pone.30727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Assay of the T cell response to antigens of My...</td>\n",
       "      <td>PLoS_One_2013_Aug_8_8(8)_e71351.txt</td>\n",
       "      <td>0.006375</td>\n",
       "      <td>http://dx.doi.org/10.1371/journal.pone.71351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Motor disability is one of the most common def...</td>\n",
       "      <td>PLoS_One_2014_Jan_8_9(1)_e84729.txt</td>\n",
       "      <td>0.010826</td>\n",
       "      <td>http://dx.doi.org/10.1371/journal.pone.84729</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                           abstract  \\\n",
       "0           0  Invasive candidiases are life threatening oppo...   \n",
       "1           1  Multiple sclerosis (MS) is a chronic inflammat...   \n",
       "2           2  There is a strong ongoing debate on the catego...   \n",
       "3           3  Assay of the T cell response to antigens of My...   \n",
       "4           4  Motor disability is one of the most common def...   \n",
       "\n",
       "                                    file  lpi_probability  \\\n",
       "0   PLoS_One_2013_Jul_23_8(7)_e69664.txt         0.268799   \n",
       "1  PLoS_One_2011_Oct_20_6(10)_e26262.txt         0.000257   \n",
       "2   PLoS_One_2012_Feb_17_7(2)_e30727.txt         0.002204   \n",
       "3    PLoS_One_2013_Aug_8_8(8)_e71351.txt         0.006375   \n",
       "4    PLoS_One_2014_Jan_8_9(1)_e84729.txt         0.010826   \n",
       "\n",
       "                                            doi  \n",
       "0  http://dx.doi.org/10.1371/journal.pone.69664  \n",
       "1  http://dx.doi.org/10.1371/journal.pone.26262  \n",
       "2  http://dx.doi.org/10.1371/journal.pone.30727  \n",
       "3  http://dx.doi.org/10.1371/journal.pone.71351  \n",
       "4  http://dx.doi.org/10.1371/journal.pone.84729  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plos_abstracts[\"doi\"] = [\"http://dx.doi.org/10.1371/journal.pone.%s\" % (filename.split(\"e\")[-1].split(\".\")[0])\n",
    "                         for filename in plos_abstracts.file.values]\n",
    "plos_abstracts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plos_abstracts[[\"abstract\", \"file\", \"lpi_probability\"]].to_csv(\"PLoS_abstract_probabilities.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
