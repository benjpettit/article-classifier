{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35938"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction import text\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import metrics\n",
    "from string import punctuation\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "DATASET = \"lpi\"\n",
    "\n",
    "sourceDir = 'PLoS_One/'\n",
    "fileList = os.listdir(sourceDir)\n",
    "len(fileList)\n",
    "\n",
    "abstractFile = \"abstracts_data/pubmed_ecology_abstracts.txt\"\n",
    "with open(abstractFile) as f:\n",
    "    negatives = ' '.join([line.strip() for line in f])\n",
    "negatives = [paragraph for paragraph in negatives.split('  ') if len(paragraph) > 300]\n",
    "random.shuffle(negatives)\n",
    "len(negatives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "691"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if DATASET.lower() == \"predicts\":\n",
    "    df = pd.read_csv('abstracts_data/unique_id_fields.csv')\n",
    "    positives = list(df['Abstract'].dropna())\n",
    "    positives = [item.lower().replace('abstract', '').replace('unavailable','').strip() for item in positives]\n",
    "elif DATASET.lower() == \"lpi\":\n",
    "    with open(\"abstracts_data/lpi_abstracts.txt\") as f:\n",
    "        positives = []\n",
    "        for line in f:\n",
    "            if line.strip():\n",
    "                positives.append(line.strip().lower().replace(\"abstract\",\"\"))\n",
    "len(positives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def split_data(ids, docs, successes, perc_train, shuffle_data):\n",
    "    \"\"\"Split the document and classification data into training and testing sets.\"\"\"\n",
    "\n",
    "    ndx_shuffle = list(range(len(docs)))\n",
    "    # Shuffle lists\n",
    "    if shuffle_data:\n",
    "        random.shuffle(ndx_shuffle)\n",
    "\n",
    "    num_train = int(len(docs) * perc_train)\n",
    "    docs_train = [docs[i].lower() for i in ndx_shuffle[:num_train]]\n",
    "    docs_test = [docs[i].lower() for i in ndx_shuffle[num_train:]]\n",
    "    successes_train = [successes[i] for i in ndx_shuffle[:num_train]]\n",
    "    successes_test = [successes[i] for i in ndx_shuffle[num_train:]]\n",
    "    ids_train = [ids[i] for i in ndx_shuffle[:num_train]]\n",
    "    ids_test = [ids[i] for i in ndx_shuffle[num_train:]]\n",
    "\n",
    "    return ids_train, ids_test, docs_train, docs_test, successes_train, successes_test\n",
    "\n",
    "\n",
    "def rm_punctuation(string, replacement='', exclude=\"'-'\"):\n",
    "    \"\"\"Remove punctuation from an input string \"\"\"\n",
    "    string = string.replace('-', ' ')  # Always replace hyphen with space\n",
    "    for p in set(list(punctuation)) - set(list(exclude)):\n",
    "        string = string.replace(p, replacement)\n",
    "\n",
    "    string = ' '.join(string.split())  # Remove excess whitespace\n",
    "    return string\n",
    "\n",
    "def train_classifier(\n",
    "        docs_success,\n",
    "        docs_background,\n",
    "        perc_train=0.7,\n",
    "        shuffle_data=True,\n",
    "        ngram_range=(1, 1),\n",
    "        filter_params={},\n",
    "        limit_data=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Train a classifier given the classification data in input_file.\n",
    "\n",
    "    @param perc_train: percentage of data points to train from input file (between 0 and 1). 0.4 by default.\n",
    "    @param shuffle_data: whether to shuffle the input data before training. True by default.\n",
    "    @return: the keywords and their corresponding coefficients as ndarrays, sorted by coefficient value.\n",
    "    \"\"\"\n",
    "\n",
    "    docs = docs_success + docs_background\n",
    "    successes = [1 for i in range(len(docs_success))] + [0 for j in range(len(docs_background))]\n",
    "    ids = range(len(docs))\n",
    "\n",
    "    # Remove punctuation from docs\n",
    "    docs = [rm_punctuation(str(_)) for _ in docs]\n",
    "\n",
    "    # Split data into training and test sets\n",
    "    ids_train, ids_test, docs_train, docs_test, y_train, y_test = \\\n",
    "        split_data(ids, docs, successes, perc_train, shuffle_data)\n",
    "\n",
    "    # Initialise vectorizer to convert text documents into matrix of token counts\n",
    "    vect = CountVectorizer(min_df=2, ngram_range=ngram_range, stop_words='english')\n",
    "    # Extract features from training dataset using sparse vectorizer\n",
    "    X_train = vect.fit_transform(docs_train)\n",
    "    print(\"LENGTH OF TEST DOCS\",len(docs_test))\n",
    "\n",
    "    # Logistic regression classifier\n",
    "    lr_classifier = LogisticRegression(penalty='l2').fit(X_train, y_train)\n",
    "\n",
    "    def get_top_feats(feature_names, classifier, plot=True, N=10, bar_height=0.5):\n",
    "        \"\"\"Sort keywords by their coefficients\"\"\"\n",
    "        sorted_feats = np.argsort(classifier.coef_[0])  # Sorted by coefficients (descending)\n",
    "        sorted_coeffs = classifier.coef_[0][sorted_feats]\n",
    "\n",
    "        return sorted_feats, sorted_coeffs\n",
    "\n",
    "    features = np.array(vect.get_feature_names())\n",
    "    feat_ids, coeffs = get_top_feats(features, lr_classifier, plot=False, N=20)\n",
    "\n",
    "    if len(docs_test) > 0:\n",
    "        X_test = vect.transform(docs_test)\n",
    "        # Predict test data\n",
    "        y_test_predicted = lr_classifier.predict(X_test)\n",
    "\n",
    "        print('Classifier has precision %.3f and recall %.3f' % \\\n",
    "            (metrics.precision_score(y_test, y_test_predicted),\n",
    "            metrics.recall_score(y_test, y_test_predicted)))\n",
    "\n",
    "        # Examples of misclassified positives/negatives\n",
    "        positive_misses = [docs_test[i] for i in range(len(docs_test)) if (y_test[i] and not y_test_predicted[i])]\n",
    "        negative_misses = [docs_test[i] for i in range(len(docs_test)) if (not y_test[i] and y_test_predicted[i])]\n",
    "\n",
    "        print('\\nSome positive misses:')\n",
    "\n",
    "        for i in range(5):\n",
    "            try:\n",
    "                print(str(i+1) + ') ' + positive_misses[i] + \"\\n\")\n",
    "            except IndexError:\n",
    "                break\n",
    "\n",
    "        print('\\nSome negative misses:')\n",
    "        for i in range(5):\n",
    "            try:\n",
    "                print(str(i+1) + ') ' + negative_misses[i] + \"\\n\")\n",
    "            except IndexError:\n",
    "                break\n",
    "        print('')\n",
    "\n",
    "    return features[feat_ids], coeffs, lr_classifier, vect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LENGTH OF TEST DOCS 808\n",
      "Classifier has precision 0.884 and recall 0.957\n",
      "\n",
      "Some positive misses:\n",
      "1) 1 the presenceabsence of a species at a particular site is the simplest form of data that can be collected during ecological field studies we used 13 years 1990 1990 of survey data to parameterize a stochastic patch occupancy model for a metapopulation of the yellow bellied marmot in colorado and investigated the significance of particular patches and the influence of site quality network characteristics and regional stochasticity on the metapopulation persistence 2 persistence of the yellow bellied marmot metapopulation was strongly dependent on the high quality colony sites and persistence probability was highly sensitive to small changes in the quality of these sites 3 a relatively small number of colony sites was ultimately responsible for the regional persistence however lower quality satellite sites also made a significant contribution to long term metapopulation persistence especially when regional stochasticity was high 4 the northern network of the marmot metapopulation was more stable compared to the southern network and the persistence of the southern network depended heavily on the northern network 5 although complex models of metapopulation dynamics may provide a more accurate description of metapopulation dynamics such models are data intensive our study one of the very few applications of stochastic patch occupancy models to a mammalian species suggests that stochastic patch occupancy models can provide important insights into metapopulation dynamics using data that are easy to collect\n",
      "\n",
      "2) the occurrence of puma concolor cougar can be confirmed by detecting physical evidence ie tracks urine markers however determining the number of pumas responsible for creating this sign is problematic we addressed this difficulty by categorizing physical evidence sign and applied this method during the puma concolor coryi florida panther project three rules were used to distinguish individuals 1 gender was determined by track size or stride length 2 time freshness was determined by known events within the past 24 hours such as wind or rain and 3 distance between individual track sets was used as an exclusionary tool to avoid over counting we evaluated accuracy by capture and by comparison to 3 other indices this method can be adapted to count other large felines\n",
      "\n",
      "3) specific pathways of the ecological impact of invasive species remain poorly known although the spread of toxic cane toads bufo marinus through tropical australia is widely believed to have caused extensive mortality of native reptiles and mammals effects of toad ingestion on native anurans have been virtually ignored our studies on the adelaide river floodplain show that the most numerous vertebrate victims of toad invasion are native tadpoles that die when they attempt to consume toad eggs we documented 11 episodes of mass mortality totalling 1300 tadpoles of 10 species in five waterbodies within a single wet season shortly after the toads invaded a causal link between toad breeding and tadpole mortality is supported by observations that 1 in at least 9 of the 11 waterbodies involved toads bred immediately prior to mortality events 2 water quality was indistinguishable from that of control ponds and tadpoles placed in that water remained healthy 3 dead tadpoles showed no sign of disease and 4 laboratory trials showed rapid 100 mortality in native tadpoles exposed to freshly laid toad eggs despite these high mortality rates toad invasion does not appear to threaten the viability of anuran populations because frogs often breed in ponds not used by toads and because density dependent growth and survival within tadpole communities mean that additional mortality may not reduce the total effective recruitment of metamorph frogs from a waterbody c 2008 elsevier ltd all rights reserved\n",
      "\n",
      "4) focuses on the question of sustainable compatibility both ecologically and economically of pastoralism and wildlife in the mara ecosystem reviews results from a comprehensive ground survey which assessed environmental factors surface water fire scars type and greenness of vegetation counted wildlife and livestock and recorded human presence including vehicles and garbage and associated land uses in the mara ecosystem in southwestern kenya 9 16 november 2002 data derived from this survey which covered about 40 of the ecosystem was compared with data from an aerial wildlife count flown the following week and data collected during an initial november 1999 ground survey that had covered about 80 of the survey area 1999 was a much drier year that 2002 and effects of rainfall differences were particularly evident in the higher quantity and quality of browse and grasses the presence of surface water the dispersal of both wild and domestic animals but fewer recent fire scars 820 elephant were counted in 2002 the majority were the reserve areas very few in the group ranches actual numbers were in the ranches koyiaki 112 ol choor oiowua 1 and other ranches 15 and in the reserve area musiara 64 sakanumi 383 and mara triangle 245 most of the elephant were in the southern part of masai mara nr well away from the northern boundary where cattle are grazed a number of elephant were along water courses as would be expected at this time of year numbers were particularly high along the ol keja gem river the authors suggest the land use pattern here is not sustainable from either the wildlife management perspective the population trend for most species is downward or the pastoralist's perspective as these people are unable to run enough stock to maintain economic needs information is needed and monitoring should continue but as importantly is the need for communiction between all stakeholders\n",
      "\n",
      "5) although it is recognized that many factors interact to cause extinctions it is difficult to consider multiple factors when investigating species declines i conducted a post hoc exploration of the major hypotheses for the decline of the allegheny woodrat neotoma magister incorporated the historical environmental changes that accompanied and preceded the decline and considered how these events may have affected the specieswhat emerges is a complicated picture involving multiple relatively minor stressors all attributable to human activities the temporal pattern of the decline is most coherent when considered from a historical perspectiveamong the factors that are likely to have affected allegheny woodrats are two exotic tree pathogens a native parasite the proliferation of human adapted competitors and habitat fragmentation in addition changes in competitive and predatory regimes appear to have influenced the timing of the collapse although the historic record cannot give definitive answers taking a synthetic historicalâ€“ ecological approach can enhance understanding of species declines\n",
      "\n",
      "\n",
      "Some negative misses:\n",
      "1) free living stages of ticks on a commercial game farm in the thabazimbi district limpopo province south africa were collected by drag sampling with flannel strips during the period september 2003 to august 2004 a total of 5 tick species was collected from 4 sites boophilus decoloratus was the most abundant species followed by amblyomma hebraeum seasonal abundance of the ticks was quantified and an optimum time to implement control measures against the ticks is proposed\n",
      "\n",
      "2) predicting population extinctions is a key element of quantitative conservation biology and population ecology although stochastic population theories have long been used to obtain theoretical distributions of population extinction times model based predictions have rarely been tested here i report results from a quantitative analysis of extinction time in 281 experimental populations of water fleas daphnia magna in variable environments to my knowledge this is the first quantitative estimate of the shape of the distribution of population extinction times based on extinction data for any species the finding that the distribution of population extinction times was extraordinarily peaked is consistent with theoretical predictions for density independent populations but inconsistent with predictions for density dependent populations the tail of the extinction time distribution was not exponential these results imply that our current theories of extinction are inadequate future work should focus on how demographic stochasticity scales with population size and effects of nonrandom variable environments on population growth and decline\n",
      "\n",
      "3) flexible time budgets allow individual animals to buffer the effects of variable food availability by allocating more time to foraging when food density decreases this trait should be especially important for marine predators that forage on patchy and ephemeral food resources we examined flexible time allocation by a long lived marine predator the common murre uria aalge using data collected in a five year study at three colonies in alaska usa with contrasting environmental conditions annual hydroacoustic surveys revealed an order of magnitude variation in food density among the 15 colony years of study we used data on parental time budgets and local prey density to test predictions from two hypotheses hypothesis a the colony attendance of seabirds varies nonlinearly with food density and hypothesis b flexible time allocation of parent murres buffers chicks against variable food availability hypothesis a was supported colony attendance by murres was positively correlated with food over a limited range of poor to moderate food densities but independent of food over a broader range of higher densities this is the first empirical evidence for a nonlinear response of a marine predator's time budget to changes in prey density predictions from hypothesis b were largely supported 1 chick feeding rates were fairly constant over a wide range of densities and only dropped below 35 meals per day at the low end of prey density and 2 there was a nonlinear relationship between chick feeding rates and time spent at the colony with chick feeding rates only declining after time at the colony by the nonbrooding parent was reduced to a minimum the ability of parents to adjust their foraging time by more than 2 hd explains why they were able to maintain chick feeding rates of more than 35 mealsd across a 10 fold range in local food density\n",
      "\n",
      "4) using collections from the years 1892 1999 i determined maximum standard length within each of 1030 populations of riffle inhabiting darters etheostoma spp representing five species from 788 sites in illinois each site contained one to four riffle inhabiting species of etheostoma based on maximum sized individuals in each collection i calculated a ratio of standard lengths for all sympatric species pairs null models were developed using random pairings of body size measurements from sites with only one species of riffle etheostoma to test whether body size ratios of sympatric species pairs are larger indicating divergence or smaller indicating convergence than ratios generated from allopatric populations results suggest that two of nine species pairs tend to converge in body size when sympatric this suggests that convergence in some cases may facilitate the persistence of similar species in diverse communities when variation in sympatric congener number is accounted for the e caeruleum e spectabile interaction results in significantly increasing size ratios as congener number increases etheostoma caeruleum and e spectabile are the most ecologically similar and evolutionarily closely related species in the dataset the divergence within this pair associated with sympatric congener number suggests that the degree of divergent character displacement may be indirectly enhanced by community structure\n",
      "\n",
      "5) to combine the rational use of marine benthic resources and economic development of small scale fishers chile passed legislation in 1991 establishing a comanagement policy that grants exclusive territorial user rights for fisheries turfs to artisanal fisher organizations in well defined inshore coastal areas known as management and exploitation areas for benthic resources meabrs in general the policy has been proclaimed a management and economic success because benthic resource abundances have increased inside meabrs in comparison with open access areas however there is a lack of studies assessing the impact of this management policy on nontargeted subtidal species and community assemblages and the policy's implications for biodiversity and conservation this study starts to fill this gap and links the allocation of turfs for benthic resources with add on conservation benefits for species that are not directly linked with the fishery policy comparative subtidal surveys inside vs outside meabrs were used to assess the effects of three meabrs on managed targeted benthic species biodiversity species richness and community assemblages in central chile surveys focused exclusively on subtidal kelp forest habitats dominated by lessonia trabeculata spanning 4 12 m in depth and with similar levels of habitat complexity the study comprised 1 quantification of kelp forest complexity 2 understory survey of sessile species 3 quantification of conspicuous benthic macroinvertebrates including those under management and 4 quantification of reef fish species inside the kelp habitat results showed population enhancement of target managed invertebrates inside meabrs moreover reef fish species were significantly more diverse and abundant inside meabrs and community assemblages of nontarget benthic invertebrates and reef fish were significantly different inside vs outside meabrs the comanagement of inshore benthic resources in chile through meabrs aims for the sustainability of invertebrate and algae stocks however our study shows that this management tool which in practice restricts access to the entire management area provides important conservation add on effects for species that are not the focus of the management policies therefore in chile the hundreds of already established meabrs could represent an important ancillary network which complements the biodiversity objectives of fully protected areas such as no take marine protected areas or others\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "features, coefficients, model, vect = train_classifier(\n",
    "    positives,\n",
    "    negatives[:2000],\n",
    "    perc_train=.7,\n",
    "    ngram_range=(1, 3)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Bottom features\n",
      "water -1.11698966594\n",
      "health -0.632240832625\n",
      "paper -0.61989555873\n",
      "analysis -0.614726106445\n",
      "control -0.603707585186\n",
      "\n",
      "Top features\n",
      "2004 0.419978915629\n",
      "year 0.467869514905\n",
      "conservation 0.478065956089\n",
      "population 0.594435780407\n",
      "abundance 0.947052859406\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nBottom features\")\n",
    "for f,c in list(zip(features,coefficients))[:5]:\n",
    "    print(f,c)\n",
    "    \n",
    "print(\"\\nTop features\")\n",
    "for f,c in list(zip(features,coefficients))[-5:]:\n",
    "    print(f,c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'20174917-1249'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datetime\n",
    "datetime.datetime.now().strftime(\"%Y%M%d-%H%M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['models/lpi_LR_model.pkl']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save model\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "joblib.dump(vect, \"models/%s_LR_vectorizer.pkl\" % (DATASET.lower()), compress=1)\n",
    "joblib.dump(model, \"models/%s_LR_model.pkl\" % (DATASET.lower()), compress=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.load(\"models/%s_LR_model.pkl\" % (DATASET.lower()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"models/%s_LR_vectorizer.pkl\" % (DATASET.lower()), 'wb') as f:\n",
    "    pickle.dump(vect, f)\n",
    "with open(\"models/%s_LR_model.pkl\" % (DATASET.lower()), 'wb') as f:\n",
    "    pickle.dump(model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(\"models/%s_LR_model.pkl\" % (DATASET.lower()), 'rb') as f:\n",
    "    model2 = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.70930308])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict_proba(vect.transform([\"hello world\"]))[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "IOError",
     "evalue": "[Errno 2] No such file or directory: 'PLoS_One/all_abstracts.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIOError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-229a760a10a1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m                         \u001b[0msuggestions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfileName\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mabstract\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcandidate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0msuggestions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0msuggestions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuggest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvect\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msuggestions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"hello\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-31-229a760a10a1>\u001b[0m in \u001b[0;36msuggest\u001b[0;34m(model, vect)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0msuggestions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mfileName\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfileList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msourceDir\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfileName\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m'introduction'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m                 \u001b[0mcandidate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIOError\u001b[0m: [Errno 2] No such file or directory: 'PLoS_One/all_abstracts.csv'"
     ]
    }
   ],
   "source": [
    "def suggest(model,vect):\n",
    "    suggestions = []\n",
    "    for fileName in fileList[:1000]:\n",
    "        with open(os.path.join(sourceDir,fileName)) as f:\n",
    "            if f.readline().strip().lower()=='introduction':\n",
    "                candidate = f.readline()\n",
    "                if len(candidate)>50:\n",
    "                    prediction = model.predict(vect.transform([rm_punctuation(str(candidate))]))\n",
    "                    if prediction[0]:\n",
    "                        suggestions.append(dict(file=fileName, abstract=candidate))\n",
    "    return suggestions\n",
    "suggestions = suggest(model,vect)\n",
    "len(suggestions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(suggestions).to_csv('suggested_{0}.csv'.format(DATASET.lower()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
